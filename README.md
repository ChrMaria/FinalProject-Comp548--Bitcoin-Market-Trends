# Big Data Management and Processing  
## Bitcoin Market Trends Analysis

## Project Overview

This project performs a Big Data analysis of Bitcoin network activity using historical daily transaction data. The objective is to demonstrate a complete Big Data workflow, including data ingestion into a NoSQL database, data cleaning and transformation, batch analytics, trend and anomaly detection, and visualization of analytical insights.

The project applies core Big Data concepts such as:

- NoSQL data modeling with MongoDB  
- Large-scale data ingestion  
- Indexing and aggregation for efficient queries  
- Batch analytics concepts inspired by MapReduce and Apache Spark  
- Analytical interpretation of trends and abnormal market activity  

All analysis is implemented in well-documented Jupyter Notebooks to ensure clarity and reproducibility.

---

## Dataset

- **Dataset:** Bitcoin daily network statistics  
- **Source:** Google Cloud BigQuery (public cryptocurrency datasets)  
- **Time Range:** 2009 â€“ 2026  
- **Format:** CSV (`btc_daily.csv`)  

### Key Attributes
- `day`  
- `tx_count`  
- `total_output_satoshis`  
- `total_fee_satoshis`  
- `avg_fee_satoshis`  

The dataset represents large-scale time-series data generated by blockchain network activity.

---

## Project Structure

This project is organized into **two Jupyter Notebooks**, each serving a distinct role in the Big Data workflow:

### 1. MongoDB-DatasetInsights.ipynb (Primary Notebook)

This notebook contains the **complete end-to-end analysis**, including:

- Data loading  
- Data cleaning and transformation  
- Ingestion into MongoDB Atlas  
- Index creation for time-series queries  
- Aggregation pipelines for trend and spike detection  
- Visualization of analytical results  

This notebook alone is sufficient to reproduce all core project results.

### 2. Spark-BatchAnalysis.ipynb (Supplementary Notebook)

This notebook demonstrates **batch analytics using Apache Spark**, illustrating distributed data processing concepts taught in the course. It is included to highlight scalability and separation between storage and processing layers.

> **Note:** The Spark notebook is optional and not required to reproduce the main analytical results.

---

## Installation & Environment Setup

### 1. Python Environment

- Python **3.9 or newer**

### 2. Required Python Packages

Install the following dependencies:

```bash
pip install pandas
pip install pymongo
pip install matplotlib
pip install pyspark

