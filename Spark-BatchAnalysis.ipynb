{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0ed151-b2dc-44dd-80b7-462df6f9a4e4",
   "metadata": {},
   "source": [
    "# Big Data Management and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ce49a-2004-46fd-91a4-5666fc205b0b",
   "metadata": {},
   "source": [
    "## Bitcoin Market Trends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0fbc5-1a38-4d5a-a3dd-ba478488871e",
   "metadata": {},
   "source": [
    "### Batch analytics using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5395da1-6a41-41d4-884d-7fbaad1002f6",
   "metadata": {},
   "source": [
    "#### Why Apache Spark?\n",
    "\n",
    "Apache Spark is used to demonstrate scalable batch analytics on large datasets.\n",
    "While the dataset can fit on a single machine, Spark illustrates how the same\n",
    "analysis could scale horizontally across a cluster without code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1dbf53-d993-4f80-acb8-29d849a8368f",
   "metadata": {},
   "source": [
    "Note: Spark execution depends on local Java and Spark configuration. If Spark fails to initialize, this notebook serves as a conceptual demonstration of batch analytics. All results are reproducible via the MongoDB-based notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd2242e-54f9-44d2-a93a-e20c78f96534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Bitcoin Big Data Analysis\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34785b7-4a32-4823-a994-2909cf366bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "spark_df = spark.read.csv(\n",
    "    r\"C:\\Users\\user\\Downloads\\btc_daily.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270d0cf9-45c5-4242-b734-0f43d6acc39a",
   "metadata": {},
   "source": [
    "#### Batch Aggregation Strategy\n",
    "\n",
    "The following transformations use Sparkâ€™s distributed DataFrame API,\n",
    "which internally applies MapReduce-style execution to group, aggregate,\n",
    "and summarize large volumes of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8efd79ad-9280-46ff-8714-46403dea0690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+--------------+\n",
      "|year|      avg_tx_count|    total_fees|\n",
      "+----+------------------+--------------+\n",
      "|2009|  91.3659217877095|     287000000|\n",
      "|2010| 507.6849315068493|    4398957094|\n",
      "|2011| 5210.315068493151|  308651762554|\n",
      "|2012|23095.765027322403|  679745946734|\n",
      "|2013|53817.098630136985| 1527463598088|\n",
      "|2014| 69215.67123287672|  463657408928|\n",
      "|2015|125134.30958904109|  820011053248|\n",
      "|2016| 225755.8005464481| 2255558583994|\n",
      "|2017| 285104.7369863014|10037068608288|\n",
      "|2018|223001.74246575343| 2570482391966|\n",
      "+----+------------------+--------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Yearly Aggregation\n",
    "from pyspark.sql.functions import year, to_date, avg, sum\n",
    "\n",
    "spark_df = spark_df.withColumn(\n",
    "    \"date\", to_date(\"day\"))\n",
    "\n",
    "yearly_trends = spark_df.groupBy(\n",
    "    year(\"date\").alias(\"year\")).agg(\n",
    "    avg(\"tx_count\").alias(\"avg_tx_count\"),\n",
    "    sum(\"total_fee_satoshis\").alias(\"total_fees\")).orderBy(\"year\")\n",
    "\n",
    "yearly_trends.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03541b81-b796-4477-9b81-379372c9a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|       day|tx_count|\n",
      "+----------+--------+\n",
      "|2024-04-23|  927010|\n",
      "|2024-09-08|  910083|\n",
      "|2024-07-21|  859629|\n",
      "|2024-05-26|  852655|\n",
      "|2024-07-23|  838977|\n",
      "|2024-05-25|  835040|\n",
      "|2024-10-21|  835011|\n",
      "|2024-07-29|  826129|\n",
      "|2024-07-22|  824999|\n",
      "|2024-11-19|  810805|\n",
      "+----------+--------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Spikes detection\n",
    "spark_df.orderBy(\n",
    "    spark_df.tx_count.desc()).select(\"day\", \"tx_count\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
